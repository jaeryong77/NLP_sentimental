{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[BASE LINE] 영문감성분석과제_2차 제출버젼",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMghBX4J3H8jIcCUYZZJH2G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeryong77/NLP_sentimental/blob/main/%5BBASE_LINE%5D_%EC%98%81%EB%AC%B8%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D%EA%B3%BC%EC%A0%9C_2%EC%B0%A8_%EC%A0%9C%EC%B6%9C%EB%B2%84%EC%A0%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xICYvs1LRw2",
        "outputId": "8f69f80e-9072-454b-ff22-c473a159b179"
      },
      "source": [
        "# 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbhlKQ2ApZqD"
      },
      "source": [
        "# Setting \r\n",
        "!pip install transformers --quiet # package installer for python\r\n",
        "\r\n",
        "# Data Set 을 가져온다 \r\n",
        "import json\r\n",
        "\r\n",
        "data = {'train': {'speaker': [], 'utterance': [], 'emotion': []},\r\n",
        "        'dev': {'speaker': [], 'utterance': [], 'emotion': []},\r\n",
        "        'test': {'speaker': [], 'utterance': [], 'emotion': []}}\r\n",
        "\r\n",
        "for dtype in ['train', 'dev', 'test']:\r\n",
        "  for dialog in json.loads(open('/content/gdrive/My Drive/Colab Notebooks/friend_eng/friends_'+dtype+'.json').read()):\r\n",
        "    for line in dialog:\r\n",
        "      data[dtype]['speaker'].append(line['speaker'])\r\n",
        "      data[dtype]['utterance'].append(line['utterance'])\r\n",
        "      data[dtype]['emotion'].append(line['emotion'])\r\n",
        "print('data load success')\r\n",
        "\r\n",
        "e2i_dict = dict((emo, i) for i, emo in enumerate(set(data['train']['emotion'])))\r\n",
        "i2e_dict = {i: e for e, i in e2i_dict.items()}\r\n",
        "print('e2i_dic success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWpR36f0L5su"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "pretrained_weights = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
        "model = BertModel.from_pretrained(pretrained_weights)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bert_tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
        "    self.bert_model = BertModel.from_pretrained(pretrained_weights)\n",
        "    self.linear = torch.nn.Linear(768, len(e2i_dict))\n",
        "\n",
        "  def forward(self, utterance):\n",
        "    tokens = self.bert_tokenizer.tokenize(utterance)\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]'] # (len)\n",
        "    ids = [tokenizer.convert_tokens_to_ids(tokens)] # (bat=1, len)\n",
        "    input_tensor = torch.tensor(ids).cuda()\n",
        "\n",
        "    hidden_tensor = self.bert_model(input_tensor)[0] # (bat, len, hid)\n",
        "    hidden_tensor = hidden_tensor[:, 0, :] # (bat, hid)\n",
        "    logit = self.linear(hidden_tensor)\n",
        "    return logit\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate(true_list, pred_list):\n",
        "  precision = precision_score(true_list, pred_list, average=None)\n",
        "  recall = recall_score(true_list, pred_list, average=None)\n",
        "  micro_f1 = f1_score(true_list, pred_list, average='micro')\n",
        "  print('precision:\\t', ['%.4f' % v for v in precision])\n",
        "  print('recall:\\t\\t', ['%.4f' % v for v in recall])\n",
        "  print('micro_f1: %.6f' % micro_f1)\n",
        "\n",
        "\n",
        "pretrained_weights = 'bert-base-uncased'\n",
        "learning_rate = 1e-5\n",
        "n_epoch = 3\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import torch\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "model = Model()\n",
        "model.cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss() # LogSoftmax & NLLLoss\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "for i_epoch in range(n_epoch):\n",
        "  print('i_epoch:', i_epoch)\n",
        "\n",
        "  model.train()\n",
        "  for i_batch in tqdm_notebook(range(len(data['train']['utterance']))):\n",
        "    logit = model(data['train']['utterance'][i_batch])\n",
        "    target = torch.tensor([e2i_dict[data['train']['emotion'][i_batch]]]).cuda()\n",
        "    loss = criterion(logit, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "  model.eval()\n",
        "  pred_list, true_list = [], []\n",
        "  for i_batch in tqdm_notebook(range(len(data['dev']['utterance']))):\n",
        "    logit = model(data['dev']['utterance'][i_batch])\n",
        "    _, max_idx = torch.max(logit, dim=-1)\n",
        "    pred_list += max_idx.tolist()\n",
        "    true_list += [e2i_dict[data['dev']['emotion'][i_batch]]]\n",
        "  print(i2e_dict)  \n",
        "  evaluate(pred_list, true_list) # print results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh1_zvo-WYhl"
      },
      "source": [
        "from collections import OrderedDict\r\n",
        "import pandas as pd\r\n",
        "en_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/competition_file/en_data.csv',encoding='utf-8')\r\n",
        "\r\n",
        "result = []\r\n",
        "labeled = []\r\n",
        "\r\n",
        "\r\n",
        "# dialogs = json.loads(open('unlabeled.json').read())\r\n",
        "for line in tqdm_notebook(en_data['utterance']):\r\n",
        "  logit = model(line)\r\n",
        "  _, max_idx = torch.max(logit, dim=-1)\r\n",
        "  pred_emotion = max_idx.tolist()[0]\r\n",
        "  result.append(i2e_dict[pred_emotion])\r\n",
        "\r\n",
        "result_data=pd.DataFrame()\r\n",
        "result_data['id'] = en_data['id']\r\n",
        "result_data['Predicted'] = result\r\n",
        "\r\n",
        "# File 생성\r\n",
        "result_data.to_csv('/content/gdrive/My Drive/Colab Notebooks/competition_file/en_result.csv', encoding='utf-8', sep=',', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwV_SMmIbr3Q"
      },
      "source": [
        "result_data=pd.DataFrame()\r\n",
        "result_data['id'] = en_data['id']\r\n",
        "result_data['Predicted'] = result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKTUPJfTacPs"
      },
      "source": [
        "# File 생성\r\n",
        "result_data.to_csv('/content/gdrive/My Drive/Colab Notebooks/competition_file/en_result.csv', encoding='utf-8', sep=',', index = False)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}